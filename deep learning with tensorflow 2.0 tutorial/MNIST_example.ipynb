{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from ipywidgets import IntProgress\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset from tensorflow datasets\n",
    "# as_supervised=True will load the dataset in 2-tuple structure [input, target]\n",
    "# with_info= True provides a tuple containing info about the version, features, # samples of the dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', as_supervised=True, with_info= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples =  tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.\n",
    "    return image, label\n",
    "# map applies a custom transformation to a given dataset\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# suffling : keeping the same information but in different order\n",
    "\n",
    "# when we are dealing with enormous dataset, we can't suffle all data at once\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# the mnist dataset is iterable and in two-tuple format(as_supervized = TRUE)\n",
    "# iter() creates an object which can be iterated one element at a time (in a for loop)\n",
    "# next() loads the next element of an iterable object\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "# Flatten: transform a tensor to a vector : (28,28,1) -> (784,1)\n",
    "# Dense: take inputs provided to the model and calculate the dot product of the inputs and the weights\n",
    "# and adds the biases. this is also where we can apply an activation fuction (it takes the output size)\n",
    "model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                                tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                                tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                                tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile configures the model for training \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 14s - loss: 0.4163 - accuracy: 0.8845 - val_loss: 0.2082 - val_accuracy: 0.9420\n",
      "Epoch 2/5\n",
      "540/540 - 13s - loss: 0.1893 - accuracy: 0.9441 - val_loss: 0.1569 - val_accuracy: 0.9557\n",
      "Epoch 3/5\n",
      "540/540 - 14s - loss: 0.1441 - accuracy: 0.9568 - val_loss: 0.1360 - val_accuracy: 0.9615\n",
      "Epoch 4/5\n",
      "540/540 - 14s - loss: 0.1177 - accuracy: 0.9650 - val_loss: 0.1183 - val_accuracy: 0.9650\n",
      "Epoch 5/5\n",
      "540/540 - 13s - loss: 0.1005 - accuracy: 0.9696 - val_loss: 0.0962 - val_accuracy: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xaa16a40ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs =NUM_EPOCHS, validation_data=(validation_inputs, validation_targets),validation_steps=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 2s 2s/step - loss: 0.1056 - accuracy: 0.9675test loss : 0.11. Test accuracy :  96.75%\n"
     ]
    }
   ],
   "source": [
    "# returns the loss value and the metrics values for the model in the test mode\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('test loss : {0:.2f}. Test accuracy : {1: .2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
