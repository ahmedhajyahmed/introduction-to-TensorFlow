{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fashion mninst example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "#tf.__version__\n",
    "#tf.test.is_gpu_available\n",
    "#tf.test.gpu_device_name\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Fashion MNIST data is available directly in the tf.keras datasets API.\n",
    "Calling load_data on this object will give you two sets of two lists, these will be the training and testing values for the graphics that contain the clothing items and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What does these values look like? Let's print a training image, and a training label to see...Experiment with different indices in the array. For example, also take a look at index 42...that's a a different boot than the one at index 0\n",
    "If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'...and fortunately in Python it's easy to normalize a list like this without looping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels  9\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
      "  0.         0.00392157 0.01568627 0.         0.         0.\n",
      "  0.         0.00392157 0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
      "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
      "  0.01568627 0.         0.         0.01176471]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
      "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
      "  0.         0.04705882 0.03921569 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
      "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
      "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.00392157\n",
      "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
      "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
      "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
      "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
      "  0.48235294 0.76862745 0.89803922 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
      "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
      "  0.8745098  0.96078431 0.67843137 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
      "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
      "  0.8627451  0.95294118 0.79215686 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
      "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
      "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02352941 0.\n",
      "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
      "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
      "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01568627 0.         0.\n",
      "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
      "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
      "  0.85098039 0.81960784 0.36078431 0.        ]\n",
      " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
      "  0.00784314 0.         0.         0.         0.         0.\n",
      "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
      "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
      "  0.85490196 1.         0.30196078 0.        ]\n",
      " [0.         0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
      "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
      "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
      "  0.87843137 0.95686275 0.62352941 0.        ]\n",
      " [0.         0.         0.         0.         0.07058824 0.17254902\n",
      "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
      "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
      "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
      "  0.91372549 0.93333333 0.84313725 0.        ]\n",
      " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
      "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
      "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
      "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
      "  0.8627451  0.90980392 0.96470588 0.        ]\n",
      " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
      "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
      "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
      "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
      "  0.87058824 0.89411765 0.88235294 0.        ]\n",
      " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
      "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
      "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
      "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
      "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
      " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
      "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
      "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
      "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
      "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
      " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
      "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
      "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
      "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
      "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
      " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
      "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
      "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
      "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
      "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
      " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
      "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
      "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
      "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
      "  0.75294118 0.84705882 0.66666667 0.        ]\n",
      " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
      "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
      "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
      "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
      "  0.38823529 0.22745098 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
      "  0.1372549  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[0])\n",
    "print(\"training labels \",train_labels[0])\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now design the model. \n",
    "Sequential: That defines a SEQUENCE of layers in the neural network\n",
    "\n",
    "Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
    "\n",
    "Dense: Adds a layer of neurons\n",
    "\n",
    "Each layer of neurons need an activation function to tell them what to do. There's lots of options, but just use these for now.\n",
    "\n",
    "Relu effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
    "\n",
    "Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
    "    tf.keras.layers.Dense(units = 128, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4981 - accuracy: 0.8247\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.3751 - accuracy: 0.8637\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3352 - accuracy: 0.8765\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3128 - accuracy: 0.8852\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.2941 - accuracy: 0.8908\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2811 - accuracy: 0.8959\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.2689 - accuracy: 0.9009\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2579 - accuracy: 0.9039\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2483 - accuracy: 0.9070\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.2395 - accuracy: 0.9107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xf681e2a630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer= 'adam', loss = 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(train_images,train_labels,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 86us/sample - loss: 0.3329 - accuracy: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3328890634059906, 0.8838]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exercices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0171505e-09 2.4052033e-08 2.8139324e-11 6.8414857e-11 1.2148168e-09\n",
      " 1.3793139e-04 5.5855494e-08 1.3678984e-02 9.5899885e-08 9.8618293e-01]\n"
     ]
    }
   ],
   "source": [
    "clasifications = model.predict(test_images)\n",
    "print(clasifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice 2( adding more units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4723\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3558\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3190\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.2945\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2769\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.3363\n",
      "[9.0486128e-06 6.3337460e-09 1.6876861e-07 3.2269805e-09 3.1600931e-07\n",
      " 6.0444307e-03 3.1029531e-06 6.2081687e-02 5.3110132e-05 9.3180811e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice 3 ( additional layers in the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.4665\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.3538\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.3208\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.2950\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.2768\n",
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.3573\n",
      "[5.0738111e-09 6.6375236e-08 3.9775741e-10 3.8513401e-09 1.1575716e-09\n",
      " 9.8087708e-04 2.1791577e-09 1.8635790e-03 2.6667959e-08 9.9715543e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(512, activation = tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice 4 (removing normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 4.5342\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5436\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5098\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5029\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4901\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.6495\n",
      "[2.7124063e-20 4.7037116e-17 6.9018113e-29 7.6150015e-12 3.4838113e-26\n",
      " 1.4326438e-04 3.7767765e-22 5.2622880e-04 6.5384623e-16 9.9933052e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "#training_images = training_images/255.0\n",
    "#test_images = test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice 5 (callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.4782 - accuracy: 0.8301\n",
      "Reached 60% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.4781 - accuracy: 0.8301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xc018858358>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.6):\n",
    "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
